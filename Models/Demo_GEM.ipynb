{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CIFAR-100 dataset...\n",
      "Extracting CIFAR-100 dataset...\n",
      "Downloading MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Paths and URLs for CIFAR-100 and MNIST datasets\n",
    "cifar_url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "cifar_path = \"cifar-100-python.tar.gz\"\n",
    "mnist_url = \"https://s3.amazonaws.com/img-datasets/mnist.npz\"\n",
    "mnist_path = \"mnist.npz\"\n",
    "\n",
    "# Download CIFAR-100 dataset if not already present\n",
    "if not os.path.exists(cifar_path):\n",
    "    print(\"Downloading CIFAR-100 dataset...\")\n",
    "    subprocess.call(f\"wget {cifar_url}\", shell=True)\n",
    "\n",
    "# Extract CIFAR-100 if not already extracted\n",
    "if not os.path.exists('cifar-100-python'):\n",
    "    print(\"Extracting CIFAR-100 dataset...\")\n",
    "    subprocess.call(f\"tar xzfv {cifar_path}\", shell=True)\n",
    "\n",
    "# Download MNIST dataset if not already present\n",
    "if not os.path.exists(mnist_path):\n",
    "    print(\"Downloading MNIST dataset...\")\n",
    "    subprocess.call(f\"wget {mnist_url}\", shell=True)\n",
    "\n",
    "# Function to unpickle CIFAR files\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "cifar100_train = unpickle('cifar-100-python/train')\n",
    "cifar100_test = unpickle('cifar-100-python/test')\n",
    "\n",
    "# Process CIFAR-100 data\n",
    "x_tr = torch.from_numpy(cifar100_train[b'data'])\n",
    "y_tr = torch.LongTensor(cifar100_train[b'fine_labels'])\n",
    "x_te = torch.from_numpy(cifar100_test[b'data'])\n",
    "y_te = torch.LongTensor(cifar100_test[b'fine_labels'])\n",
    "\n",
    "# Save processed CIFAR-100 dataset\n",
    "torch.save((x_tr, y_tr, x_te, y_te), 'cifar100.pt')\n",
    "\n",
    "# Load MNIST dataset\n",
    "f = np.load(mnist_path)\n",
    "x_tr_mnist = torch.from_numpy(f['x_train'])\n",
    "y_tr_mnist = torch.from_numpy(f['y_train']).long()\n",
    "x_te_mnist = torch.from_numpy(f['x_test'])\n",
    "y_te_mnist = torch.from_numpy(f['y_test']).long()\n",
    "f.close()\n",
    "\n",
    "# Save processed MNIST dataset\n",
    "torch.save((x_tr_mnist, y_tr_mnist), 'mnist_train.pt')\n",
    "torch.save((x_te_mnist, y_te_mnist), 'mnist_test.pt')\n",
    "\n",
    "# Argument parsing setup for task split\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--i', default='cifar100.pt', help='input directory')\n",
    "parser.add_argument('--o', default='cifar100_tasks.pt', help='output file')\n",
    "parser.add_argument('--n_tasks', default=10, type=int, help='number of tasks')\n",
    "parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
    "\n",
    "# Handle Jupyter's extra arguments\n",
    "if 'ipykernel' in sys.modules:\n",
    "    args = parser.parse_args([])\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Initialize lists for training and test tasks\n",
    "tasks_tr = []\n",
    "tasks_te = []\n",
    "\n",
    "# Load data and preprocess\n",
    "x_tr, y_tr, x_te, y_te = torch.load(os.path.join(args.i))\n",
    "x_tr = x_tr.float().view(x_tr.size(0), -1) / 255.0\n",
    "x_te = x_te.float().view(x_te.size(0), -1) / 255.0\n",
    "\n",
    "# Split data into tasks\n",
    "cpt = int(100 / args.n_tasks)\n",
    "for t in range(args.n_tasks):\n",
    "    c1 = t * cpt\n",
    "    c2 = (t + 1) * cpt\n",
    "    i_tr = ((y_tr >= c1) & (y_tr < c2)).nonzero().view(-1)\n",
    "    i_te = ((y_te >= c1) & (y_te < c2)).nonzero().view(-1)\n",
    "    tasks_tr.append([(c1, c2), x_tr[i_tr].clone(), y_tr[i_tr].clone()])\n",
    "    tasks_te.append([(c1, c2), x_te[i_te].clone(), y_te[i_te].clone()])\n",
    "\n",
    "# Save the processed tasks\n",
    "torch.save([tasks_tr, tasks_te], args.o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes, nf):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = nf\n",
    "\n",
    "        self.conv1 = conv3x3(3, nf * 1)\n",
    "        self.bn1 = nn.BatchNorm2d(nf * 1)\n",
    "        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz = x.size(0)\n",
    "        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(nclasses, nf=20):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], nclasses, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Task 1, Test on Task 1, Accuracy: 49.40%\n",
      "Task 2, Test on Task 1, Accuracy: 52.20%\n",
      "Task 2, Test on Task 2, Accuracy: 43.20%\n",
      "Task 3, Test on Task 1, Accuracy: 55.40%\n",
      "Task 3, Test on Task 2, Accuracy: 42.40%\n",
      "Task 3, Test on Task 3, Accuracy: 42.80%\n",
      "Task 4, Test on Task 1, Accuracy: 55.60%\n",
      "Task 4, Test on Task 2, Accuracy: 45.00%\n",
      "Task 4, Test on Task 3, Accuracy: 46.80%\n",
      "Task 4, Test on Task 4, Accuracy: 48.80%\n",
      "Task 5, Test on Task 1, Accuracy: 58.80%\n",
      "Task 5, Test on Task 2, Accuracy: 45.80%\n",
      "Task 5, Test on Task 3, Accuracy: 47.60%\n",
      "Task 5, Test on Task 4, Accuracy: 47.60%\n",
      "Task 5, Test on Task 5, Accuracy: 57.60%\n",
      "Task 6, Test on Task 1, Accuracy: 59.20%\n",
      "Task 6, Test on Task 2, Accuracy: 45.20%\n",
      "Task 6, Test on Task 3, Accuracy: 48.60%\n",
      "Task 6, Test on Task 4, Accuracy: 47.00%\n",
      "Task 6, Test on Task 5, Accuracy: 58.00%\n",
      "Task 6, Test on Task 6, Accuracy: 45.00%\n",
      "Task 7, Test on Task 1, Accuracy: 57.60%\n",
      "Task 7, Test on Task 2, Accuracy: 47.40%\n",
      "Task 7, Test on Task 3, Accuracy: 49.80%\n",
      "Task 7, Test on Task 4, Accuracy: 48.40%\n",
      "Task 7, Test on Task 5, Accuracy: 58.20%\n",
      "Task 7, Test on Task 6, Accuracy: 41.80%\n",
      "Task 7, Test on Task 7, Accuracy: 44.40%\n",
      "Task 8, Test on Task 1, Accuracy: 56.20%\n",
      "Task 8, Test on Task 2, Accuracy: 49.40%\n",
      "Task 8, Test on Task 3, Accuracy: 50.20%\n",
      "Task 8, Test on Task 4, Accuracy: 46.60%\n",
      "Task 8, Test on Task 5, Accuracy: 58.00%\n",
      "Task 8, Test on Task 6, Accuracy: 41.40%\n",
      "Task 8, Test on Task 7, Accuracy: 43.40%\n",
      "Task 8, Test on Task 8, Accuracy: 51.00%\n",
      "Task 9, Test on Task 1, Accuracy: 54.80%\n",
      "Task 9, Test on Task 2, Accuracy: 49.20%\n",
      "Task 9, Test on Task 3, Accuracy: 49.60%\n",
      "Task 9, Test on Task 4, Accuracy: 44.00%\n",
      "Task 9, Test on Task 5, Accuracy: 58.60%\n",
      "Task 9, Test on Task 6, Accuracy: 42.80%\n",
      "Task 9, Test on Task 7, Accuracy: 44.00%\n",
      "Task 9, Test on Task 8, Accuracy: 50.60%\n",
      "Task 9, Test on Task 9, Accuracy: 48.60%\n",
      "Task 10, Test on Task 1, Accuracy: 54.20%\n",
      "Task 10, Test on Task 2, Accuracy: 46.60%\n",
      "Task 10, Test on Task 3, Accuracy: 47.60%\n",
      "Task 10, Test on Task 4, Accuracy: 46.20%\n",
      "Task 10, Test on Task 5, Accuracy: 55.20%\n",
      "Task 10, Test on Task 6, Accuracy: 39.20%\n",
      "Task 10, Test on Task 7, Accuracy: 43.80%\n",
      "Task 10, Test on Task 8, Accuracy: 48.80%\n",
      "Task 10, Test on Task 9, Accuracy: 50.00%\n",
      "Task 10, Test on Task 10, Accuracy: 57.20%\n",
      "Task 11, Test on Task 1, Accuracy: 52.80%\n",
      "Task 11, Test on Task 2, Accuracy: 48.40%\n",
      "Task 11, Test on Task 3, Accuracy: 46.60%\n",
      "Task 11, Test on Task 4, Accuracy: 44.80%\n",
      "Task 11, Test on Task 5, Accuracy: 55.60%\n",
      "Task 11, Test on Task 6, Accuracy: 37.20%\n",
      "Task 11, Test on Task 7, Accuracy: 38.20%\n",
      "Task 11, Test on Task 8, Accuracy: 49.80%\n",
      "Task 11, Test on Task 9, Accuracy: 48.00%\n",
      "Task 11, Test on Task 10, Accuracy: 60.80%\n",
      "Task 11, Test on Task 11, Accuracy: 65.20%\n",
      "Task 12, Test on Task 1, Accuracy: 52.40%\n",
      "Task 12, Test on Task 2, Accuracy: 47.20%\n",
      "Task 12, Test on Task 3, Accuracy: 47.80%\n",
      "Task 12, Test on Task 4, Accuracy: 41.60%\n",
      "Task 12, Test on Task 5, Accuracy: 53.80%\n",
      "Task 12, Test on Task 6, Accuracy: 41.00%\n",
      "Task 12, Test on Task 7, Accuracy: 37.60%\n",
      "Task 12, Test on Task 8, Accuracy: 47.60%\n",
      "Task 12, Test on Task 9, Accuracy: 44.80%\n",
      "Task 12, Test on Task 10, Accuracy: 60.80%\n",
      "Task 12, Test on Task 11, Accuracy: 62.80%\n",
      "Task 12, Test on Task 12, Accuracy: 43.40%\n",
      "Task 13, Test on Task 1, Accuracy: 48.80%\n",
      "Task 13, Test on Task 2, Accuracy: 45.40%\n",
      "Task 13, Test on Task 3, Accuracy: 45.40%\n",
      "Task 13, Test on Task 4, Accuracy: 46.40%\n",
      "Task 13, Test on Task 5, Accuracy: 51.00%\n",
      "Task 13, Test on Task 6, Accuracy: 37.00%\n",
      "Task 13, Test on Task 7, Accuracy: 44.80%\n",
      "Task 13, Test on Task 8, Accuracy: 50.80%\n",
      "Task 13, Test on Task 9, Accuracy: 42.20%\n",
      "Task 13, Test on Task 10, Accuracy: 58.00%\n",
      "Task 13, Test on Task 11, Accuracy: 66.20%\n",
      "Task 13, Test on Task 12, Accuracy: 45.40%\n",
      "Task 13, Test on Task 13, Accuracy: 64.20%\n",
      "Task 14, Test on Task 1, Accuracy: 47.20%\n",
      "Task 14, Test on Task 2, Accuracy: 43.40%\n",
      "Task 14, Test on Task 3, Accuracy: 42.40%\n",
      "Task 14, Test on Task 4, Accuracy: 41.20%\n",
      "Task 14, Test on Task 5, Accuracy: 52.60%\n",
      "Task 14, Test on Task 6, Accuracy: 40.40%\n",
      "Task 14, Test on Task 7, Accuracy: 40.80%\n",
      "Task 14, Test on Task 8, Accuracy: 46.40%\n",
      "Task 14, Test on Task 9, Accuracy: 41.40%\n",
      "Task 14, Test on Task 10, Accuracy: 57.80%\n",
      "Task 14, Test on Task 11, Accuracy: 58.60%\n",
      "Task 14, Test on Task 12, Accuracy: 40.80%\n",
      "Task 14, Test on Task 13, Accuracy: 64.00%\n",
      "Task 14, Test on Task 14, Accuracy: 52.60%\n",
      "Task 15, Test on Task 1, Accuracy: 46.80%\n",
      "Task 15, Test on Task 2, Accuracy: 43.00%\n",
      "Task 15, Test on Task 3, Accuracy: 45.00%\n",
      "Task 15, Test on Task 4, Accuracy: 39.40%\n",
      "Task 15, Test on Task 5, Accuracy: 48.00%\n",
      "Task 15, Test on Task 6, Accuracy: 40.40%\n",
      "Task 15, Test on Task 7, Accuracy: 42.20%\n",
      "Task 15, Test on Task 8, Accuracy: 50.00%\n",
      "Task 15, Test on Task 9, Accuracy: 43.60%\n",
      "Task 15, Test on Task 10, Accuracy: 57.20%\n",
      "Task 15, Test on Task 11, Accuracy: 60.60%\n",
      "Task 15, Test on Task 12, Accuracy: 46.80%\n",
      "Task 15, Test on Task 13, Accuracy: 65.00%\n",
      "Task 15, Test on Task 14, Accuracy: 51.60%\n",
      "Task 15, Test on Task 15, Accuracy: 64.60%\n",
      "Task 16, Test on Task 1, Accuracy: 48.00%\n",
      "Task 16, Test on Task 2, Accuracy: 43.60%\n",
      "Task 16, Test on Task 3, Accuracy: 43.20%\n",
      "Task 16, Test on Task 4, Accuracy: 40.40%\n",
      "Task 16, Test on Task 5, Accuracy: 49.60%\n",
      "Task 16, Test on Task 6, Accuracy: 37.60%\n",
      "Task 16, Test on Task 7, Accuracy: 40.20%\n",
      "Task 16, Test on Task 8, Accuracy: 49.60%\n",
      "Task 16, Test on Task 9, Accuracy: 41.00%\n",
      "Task 16, Test on Task 10, Accuracy: 61.60%\n",
      "Task 16, Test on Task 11, Accuracy: 61.20%\n",
      "Task 16, Test on Task 12, Accuracy: 46.00%\n",
      "Task 16, Test on Task 13, Accuracy: 61.20%\n",
      "Task 16, Test on Task 14, Accuracy: 45.20%\n",
      "Task 16, Test on Task 15, Accuracy: 62.40%\n",
      "Task 16, Test on Task 16, Accuracy: 51.20%\n",
      "Task 17, Test on Task 1, Accuracy: 41.60%\n",
      "Task 17, Test on Task 2, Accuracy: 42.80%\n",
      "Task 17, Test on Task 3, Accuracy: 42.80%\n",
      "Task 17, Test on Task 4, Accuracy: 44.80%\n",
      "Task 17, Test on Task 5, Accuracy: 44.80%\n",
      "Task 17, Test on Task 6, Accuracy: 38.60%\n",
      "Task 17, Test on Task 7, Accuracy: 41.80%\n",
      "Task 17, Test on Task 8, Accuracy: 47.80%\n",
      "Task 17, Test on Task 9, Accuracy: 42.60%\n",
      "Task 17, Test on Task 10, Accuracy: 57.60%\n",
      "Task 17, Test on Task 11, Accuracy: 61.20%\n",
      "Task 17, Test on Task 12, Accuracy: 47.60%\n",
      "Task 17, Test on Task 13, Accuracy: 64.20%\n",
      "Task 17, Test on Task 14, Accuracy: 48.80%\n",
      "Task 17, Test on Task 15, Accuracy: 60.00%\n",
      "Task 17, Test on Task 16, Accuracy: 48.40%\n",
      "Task 17, Test on Task 17, Accuracy: 55.20%\n",
      "Task 18, Test on Task 1, Accuracy: 45.60%\n",
      "Task 18, Test on Task 2, Accuracy: 42.00%\n",
      "Task 18, Test on Task 3, Accuracy: 46.20%\n",
      "Task 18, Test on Task 4, Accuracy: 45.20%\n",
      "Task 18, Test on Task 5, Accuracy: 51.80%\n",
      "Task 18, Test on Task 6, Accuracy: 42.00%\n",
      "Task 18, Test on Task 7, Accuracy: 46.00%\n",
      "Task 18, Test on Task 8, Accuracy: 47.00%\n",
      "Task 18, Test on Task 9, Accuracy: 44.60%\n",
      "Task 18, Test on Task 10, Accuracy: 56.40%\n",
      "Task 18, Test on Task 11, Accuracy: 61.00%\n",
      "Task 18, Test on Task 12, Accuracy: 43.40%\n",
      "Task 18, Test on Task 13, Accuracy: 60.80%\n",
      "Task 18, Test on Task 14, Accuracy: 51.00%\n",
      "Task 18, Test on Task 15, Accuracy: 60.80%\n",
      "Task 18, Test on Task 16, Accuracy: 47.40%\n",
      "Task 18, Test on Task 17, Accuracy: 55.60%\n",
      "Task 18, Test on Task 18, Accuracy: 50.40%\n",
      "Task 19, Test on Task 1, Accuracy: 43.00%\n",
      "Task 19, Test on Task 2, Accuracy: 44.60%\n",
      "Task 19, Test on Task 3, Accuracy: 47.20%\n",
      "Task 19, Test on Task 4, Accuracy: 44.20%\n",
      "Task 19, Test on Task 5, Accuracy: 45.40%\n",
      "Task 19, Test on Task 6, Accuracy: 39.40%\n",
      "Task 19, Test on Task 7, Accuracy: 42.20%\n",
      "Task 19, Test on Task 8, Accuracy: 45.20%\n",
      "Task 19, Test on Task 9, Accuracy: 41.40%\n",
      "Task 19, Test on Task 10, Accuracy: 55.40%\n",
      "Task 19, Test on Task 11, Accuracy: 60.00%\n",
      "Task 19, Test on Task 12, Accuracy: 48.20%\n",
      "Task 19, Test on Task 13, Accuracy: 59.20%\n",
      "Task 19, Test on Task 14, Accuracy: 46.20%\n",
      "Task 19, Test on Task 15, Accuracy: 58.80%\n",
      "Task 19, Test on Task 16, Accuracy: 45.60%\n",
      "Task 19, Test on Task 17, Accuracy: 58.80%\n",
      "Task 19, Test on Task 18, Accuracy: 49.80%\n",
      "Task 19, Test on Task 19, Accuracy: 59.80%\n",
      "Task 20, Test on Task 1, Accuracy: 42.20%\n",
      "Task 20, Test on Task 2, Accuracy: 43.00%\n",
      "Task 20, Test on Task 3, Accuracy: 45.80%\n",
      "Task 20, Test on Task 4, Accuracy: 41.60%\n",
      "Task 20, Test on Task 5, Accuracy: 47.40%\n",
      "Task 20, Test on Task 6, Accuracy: 41.60%\n",
      "Task 20, Test on Task 7, Accuracy: 44.80%\n",
      "Task 20, Test on Task 8, Accuracy: 48.20%\n",
      "Task 20, Test on Task 9, Accuracy: 40.60%\n",
      "Task 20, Test on Task 10, Accuracy: 55.60%\n",
      "Task 20, Test on Task 11, Accuracy: 60.80%\n",
      "Task 20, Test on Task 12, Accuracy: 43.60%\n",
      "Task 20, Test on Task 13, Accuracy: 61.20%\n",
      "Task 20, Test on Task 14, Accuracy: 50.60%\n",
      "Task 20, Test on Task 15, Accuracy: 60.60%\n",
      "Task 20, Test on Task 16, Accuracy: 44.60%\n",
      "Task 20, Test on Task 17, Accuracy: 57.00%\n",
      "Task 20, Test on Task 18, Accuracy: 47.20%\n",
      "Task 20, Test on Task 19, Accuracy: 59.80%\n",
      "Task 20, Test on Task 20, Accuracy: 58.60%\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# Auxiliary GEM functions for handling gradients\n",
    "import quadprog  # Required for GEM's gradient projection\n",
    "\n",
    "def store_grad(pp, grads, grad_dims, tid):\n",
    "    grads[:, tid].fill_(0.0)\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            grads[beg: en, tid].copy_(param.grad.data.view(-1))\n",
    "        cnt += 1\n",
    "\n",
    "def overwrite_grad(pp, newgrad, grad_dims):\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            this_grad = newgrad[beg: en].contiguous().view(param.grad.data.size())\n",
    "            param.grad.data.copy_(this_grad)\n",
    "        cnt += 1\n",
    "\n",
    "def project2cone2(gradient, memories, margin=0.5, eps=1e-3):\n",
    "    memories_np = memories.cpu().t().double().numpy()\n",
    "    gradient_np = gradient.cpu().contiguous().view(-1).double().numpy()\n",
    "    t = memories_np.shape[0]\n",
    "    P = np.dot(memories_np, memories_np.transpose())\n",
    "    P = 0.5 * (P + P.transpose()) + np.eye(t) * eps\n",
    "    q = np.dot(memories_np, gradient_np) * -1\n",
    "    G = np.eye(t)\n",
    "    h = np.zeros(t) + margin\n",
    "    v = quadprog.solve_qp(P, q, G, h)[0]\n",
    "    x = np.dot(v, memories_np) + gradient_np\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "\n",
    "# GEM Model Definition\n",
    "class GEM(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, n_tasks, args):\n",
    "        super(GEM, self).__init__()\n",
    "        self.margin = args.memory_strength\n",
    "        self.net = ResNet18(n_outputs)  # Assuming ResNet18 is defined for CIFAR-100 tasks\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.opt = optim.SGD(self.parameters(), lr=args.lr)\n",
    "        \n",
    "        self.n_memories = args.n_memories\n",
    "        self.memory_data = torch.FloatTensor(n_tasks, self.n_memories, n_inputs)\n",
    "        self.memory_labs = torch.LongTensor(n_tasks, self.n_memories)\n",
    "        self.gpu = args.cuda\n",
    "\n",
    "        # Allocate gradient storage\n",
    "        self.grad_dims = [param.data.numel() for param in self.parameters()]\n",
    "        self.grads = torch.Tensor(sum(self.grad_dims), n_tasks)\n",
    "        if args.cuda:\n",
    "            self.memory_data = self.memory_data.cuda()\n",
    "            self.memory_labs = self.memory_labs.cuda()\n",
    "            self.grads = self.grads.cuda()\n",
    "\n",
    "        self.observed_tasks = []\n",
    "        self.old_task = -1\n",
    "        self.mem_cnt = 0\n",
    "        self.nc_per_task = n_outputs // n_tasks\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        output = self.net(x)\n",
    "        offset1, offset2 = t * self.nc_per_task, (t + 1) * self.nc_per_task\n",
    "        output[:, :offset1].data.fill_(-10e10)\n",
    "        output[:, offset2:].data.fill_(-10e10)\n",
    "        return output\n",
    "\n",
    "    def observe(self, x, t, y):\n",
    "        if t != self.old_task:\n",
    "            self.observed_tasks.append(t)\n",
    "            self.old_task = t\n",
    "\n",
    "        bsz = y.size(0)\n",
    "        endcnt = min(self.mem_cnt + bsz, self.n_memories)\n",
    "        effbsz = endcnt - self.mem_cnt\n",
    "        self.memory_data[t, self.mem_cnt:endcnt].copy_(x[:effbsz])\n",
    "        self.memory_labs[t, self.mem_cnt:endcnt].copy_(y[:effbsz])\n",
    "        self.mem_cnt += effbsz\n",
    "        if self.mem_cnt == self.n_memories:\n",
    "            self.mem_cnt = 0\n",
    "\n",
    "        if len(self.observed_tasks) > 1:\n",
    "            for past_task in self.observed_tasks[:-1]:\n",
    "                self.zero_grad()\n",
    "                ptloss = self.ce(self.forward(self.memory_data[past_task], past_task),\n",
    "                                 self.memory_labs[past_task])\n",
    "                ptloss.backward()\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims, past_task)\n",
    "\n",
    "        self.zero_grad()\n",
    "        loss = self.ce(self.forward(x, t), y)\n",
    "        loss.backward()\n",
    "\n",
    "        if len(self.observed_tasks) > 1:\n",
    "            store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "            indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu else torch.LongTensor(self.observed_tasks[:-1])\n",
    "            dotp = torch.mm(self.grads[:, t].unsqueeze(0), self.grads.index_select(1, indx))\n",
    "            if (dotp < 0).sum() != 0:\n",
    "                project2cone2(self.grads[:, t].unsqueeze(1), self.grads.index_select(1, indx), self.margin)\n",
    "                overwrite_grad(self.parameters, self.grads[:, t], self.grad_dims)\n",
    "        self.opt.step()\n",
    "\n",
    "# CIFAR-100 Task Loader and Evaluation\n",
    "def load_cifar100_tasks(args):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    train_data = datasets.CIFAR100(root=args.data_path, train=True, download=True, transform=transform)\n",
    "    test_data = datasets.CIFAR100(root=args.data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "    classes_per_task = 100 // args.n_tasks\n",
    "    train_tasks, test_tasks = [], []\n",
    "    \n",
    "    for task in range(args.n_tasks):\n",
    "        task_classes = range(task * classes_per_task, (task + 1) * classes_per_task)\n",
    "        train_idx = [i for i, target in enumerate(train_data.targets) if target in task_classes]\n",
    "        test_idx = [i for i, target in enumerate(test_data.targets) if target in task_classes]\n",
    "        \n",
    "        train_tasks.append(DataLoader(Subset(train_data, train_idx), batch_size=args.batch_size, shuffle=True))\n",
    "        test_tasks.append(DataLoader(Subset(test_data, test_idx), batch_size=args.batch_size, shuffle=False))\n",
    "    \n",
    "    return train_tasks, test_tasks\n",
    "\n",
    "# Training and Evaluation Process\n",
    "def train_gem_on_cifar100(args):\n",
    "    device = torch.device(\"cuda:2\" if args.cuda else \"cpu\")\n",
    "    train_tasks, test_tasks = load_cifar100_tasks(args)\n",
    "    model = GEM(32 * 32 * 3, 100, args.n_tasks, args).to(device)  # CIFAR-100 has 32x32x3 inputs and 100 classes\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    task_accuracies = []\n",
    "    for task_id, (train_loader, test_loader) in enumerate(zip(train_tasks, test_tasks)):\n",
    "        model.train()\n",
    "        for epoch in range(args.n_epochs):\n",
    "            for x, y in train_loader:\n",
    "                x = x.view(x.size(0), -1)\n",
    "                if args.cuda:\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                model.observe(x, task_id, y)\n",
    "\n",
    "        # Evaluate on all observed tasks\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        for test_id, test_loader in enumerate(test_tasks[:task_id + 1]):\n",
    "            correct, total = 0, 0\n",
    "            for x, y in test_loader:\n",
    "                x = x.view(x.size(0), -1)\n",
    "                if args.cuda:\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                with torch.no_grad():\n",
    "                    output = model(x, test_id)\n",
    "                    _, pred = output.max(1)\n",
    "                    correct += pred.eq(y).sum().item()\n",
    "                    total += y.size(0)\n",
    "            accuracy = 100 * correct / total\n",
    "            accuracies.append(accuracy)\n",
    "            print(f\"Task {task_id+1}, Test on Task {test_id+1}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        task_accuracies.append(accuracies)\n",
    "\n",
    "    return task_accuracies\n",
    "\n",
    "# Set arguments\n",
    "class Args:\n",
    "    data_path = './data'\n",
    "    model = 'gem'\n",
    "    n_tasks = 20\n",
    "    n_memories = 200\n",
    "    memory_strength = 0.5\n",
    "    n_epochs = 1\n",
    "    batch_size = 10\n",
    "    lr = 1e-3\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "args = Args()\n",
    "task_accuracies = train_gem_on_cifar100(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
